#
# Z_resolution.py -- Zelevinsky Resolution class
# Author: Benjamin F Jones
# Version: 7-13-08
#

# 
# Imports
# 

#system
import networkx
from copy import *
from sage.combinat.combinat import combinations_iterator
from sage.misc.mrange import cartesian_product_iterator
from sage.combinat.sf.sfa import SFASchur
# custom
from csm_partition import *
from bott_residue import ChernClass, ChernClassMonomial

#
# Classes
#

class Zresolution:
    """
    A class to implement the combinatorial aspects of Zelevinsky
    resolutions.
    """
    def __init__(self, part, perm):
        # simple data
        self.num_nodes = len(perm)
        self.ic_size = len(perm)+1

        # data  generated by procedure
        self.graph = self.incidence_graph( perm )
        self.weights = self.ig_weights( part, perm )
        self.ic_fix = [ map(lambda x:x+1, range(i)) for i in self.weights[:self.ic_size] ]
        self.bounds = []
        self.set_bounds()
        self.part = part
        self.perm = perm
        self.dimension = sum( part )
        self.blocks_computed = False
    
    def incidence_graph(self, plist):
        """Creates an incidence graph from: n = 'initial chain size'
        and plist = 'a permutation'"""
        n = self.ic_size
        # First, create the inital chain
        d = {}
        for i in range(n-1):
            d.update( { i:[i+1] } )
        g = networkx.DiGraph(d)
        # node list
        nlist = range(n)
        # Next, add extra nodes
        for i in range(self.num_nodes):
            ni = i + n
            p = plist[i]
            # add new nodes and edges
            g.add_node(ni)
            g.add_path([ nlist[p-1], ni, nlist[p] ])
            # modify the node list
            nlist[p-1:p+1] = [ni]
        return g
    
    def ig_weights(self, part, plist):
        """Returns a list of weights, one for each node (in order)
        of the graph incidence_graph( n_peaks, plist)."""
        part_list = [ part ]
        z = [ to_zel_part(part) ]
        # initial weights
        w = [ [ sum(z[-1][0][:i]) + sum(z[-1][1][:i]) for i in range(len(z[-1][0])+1) ] ]
        # weights for the nodes off the initial chain
        for j in range(len(plist)):
            part_list.append( remove_peak( part_list[-1], plist[j] ) )
            z.append( to_zel_part( part_list[-1] ) )
            # shift weights more if the 1st peak is removed
            if plist[j] == 1:
                shift = w[-1][0] + z[-2][0][0]
            # standard shift
            else:
                shift = w[-1][0]
            w.append([ sum(z[-1][0][:i]) + sum(z[-1][1][:i]) + shift \
                       for i in range(len(z[-1][0])+1) ])
        # weights for the nodes in initial chain
        d = copy(w[0])
        # weights for the nodes below the chain
        for i in range(len(plist)):
            d.append(w[i+1][plist[i]-1])
        return d

        
    def set_bounds(self):
        """
        Sets the nearest predecessor and successor nodes for each node
        in the incidence graph off the initial chain
        """
        for i in range(self.num_nodes):
            pred_list = self.graph.predecessors(i+self.ic_size)
            succ_list = self.graph.successors(i+self.ic_size)
            pred_list = [ j for j in pred_list if j < i+self.ic_size ]
            succ_list = [ j for j in succ_list if j < i+self.ic_size ]
            self.bounds.append( (pred_list[0], succ_list[0]) )
    
    def get_bounds(self, i):
        """Return the first predecessor and successor of node i in g among
           all those nodes connected to i with smaller node number. Here, 'i' 
           can range from 0 to (num_nodes-1)
        """
        if i in range(self.num_nodes):
            return self.bounds[i]
        else:
            raise IndexError, 'node %d is out of bounds' % i
    
    #################################
    # Functions specific to T-fixed
    # points and localization
    #################################
    
    def fixed_points(self, i=0):
        """Returns a list of fixed points of the (projection of the) incidence
        variety assoc. to g,w (where we project to include only nodes 1..i). This
        allows us to build a list of fixed points for the whole incidence variety
        inductively.
        
        INPUT:
            i = How many nodes to include
                This defaults to i = num_nodes (or if i=0)
            (from self)
            g = incidence graph (result of calling incidence_graph(n,plist)
            w = weights for the nodes (result of calling ig_weights(part,plist)
                for the same plist)
            n = length of init. chain (optional). defaults to (len(w)+1)/2
        
        OUTPUT: [ fp_1, fp_2, ... ] where each fp_i is itself a list of
        index sets, one for each node from 1..i. An index set is a list of
        indices which determines the character of the $T$ action on the
        fiber of the corresponding universal bundle at the fixed point
        fp_i.
        
        NOTES:
        Tends to bog down around 8 levels of recursion, i.e.
        p=[8,7,6,5,4,3,2,1] # part and plist will be the same here
        g=incidence_graph(9,p)
        w=ig_weights(p,p)
        FP = fixed_points(g,w,9,7)   # runs for 10 sec
        FP = fixed_points(g,w,9,8)   # runs for a LONG time
        
        or...
        
        sage: plist=[8,7,6,5,4,3,2,1]
        sage: g = incidence_graph(plist)
        sage: w = ig_weights(plist,plist)
        sage: time FP2 = fixed_points(g,w,len(plist)+1,len(plist)-2)
        sage: time FP1 = fixed_points(g,w,len(plist)+1,len(plist)-1)
        sage: time FP0 = fixed_points(g,w,len(plist)+1,len(plist))
        Time: CPU 0.54 s, Wall: 0.54 s
        Time: CPU 9.14 s, Wall: 9.15 s
        Time: CPU 557.46 s, Wall: 558.66 s
        sage: len(FP2)
        5040
        sage: len(FP1)
        40320
        sage: len(FP0)
        362880
        """
        
        # conversion of names to class context
        g = self.graph
        w = self.weights
        n = self.ic_size
        
        # i defaults to 0 -- meaning i = num_nodes
        if i == 0:
            i = self.num_nodes
        
        (pred,succ) = self.bounds[i-1]
        node_weight = w[n+i-1]
        pred_weight = w[pred]
        weight_diff = node_weight - pred_weight
        
        # base of induction
        if i == 1:
            pred_index_set = set(self.ic_fix[pred])
            succ_index_set = set(self.ic_fix[succ])
            diff_index_set = succ_index_set.difference(pred_index_set)
            C = combinations_iterator( list(diff_index_set), weight_diff )
            poss = [ [self.ic_fix[pred]+c]  for c in C ]
            return poss
        elif i > 1:
            # get the result for i-1
            induct = self.fixed_points(i-1)
            poss = []
            for fp in induct:
                fpp = self.ic_fix + fp  # tack on the initial data
                pred_index_set = set(fpp[pred])
                succ_index_set = set(fpp[succ])
                diff_index_set = succ_index_set.difference(pred_index_set)
                C = combinations_iterator( list(diff_index_set), weight_diff )
                newfps = [ fp+[fpp[pred]+c] for c in C ]
                poss = poss + newfps
            return poss
        else:
            return None
    
    def fixed_points_iterator(self, i=0):
        """Same as fixed_points() but acts as an interator.
        See doc string for fixed_points().
        
        NOTES: The iterator takes dramatically less time than
        fixed_points() when i gets large. For example:
        
        sage: ## TEST THE ITERATOR
        sage: plist=[8,7,6,5,4,3,2,1]
        sage: g = incidence_graph(plist)
        sage: w = ig_weights(plist,plist)
        sage: FP = fixed_points_iterator(g,w,len(plist)+1,len(plist))
        sage: t0 = cputime()
        sage: count=0
        sage: for i,fp in enumerate(FP):
        ...       if i%50000 == 0:
        ...           print cputime()-t0,",",
        ...
        sage: print "Total Time (iterator):",cputime()-t0
        0.000598000000011 , 6.469392 , 12.936641 , 19.405214 ,
        25.886539 , 32.357304 , 38.827102 , 45.297906 , Total Time
        (iterator): 46.964347
        
        Compare to 557.46 s for the same call to fixed_points()
        """
        
        # conversion of names to class context
        g = self.graph
        w = self.weights
        n = self.ic_size
        
        if i == 0:
            i = self.num_nodes
        
        (pred,succ) = self.bounds[i-1]
        node_weight = w[n+i-1]
        pred_weight = w[pred]
        weight_diff = node_weight - pred_weight
        
        # base of induction
        if i == 1:
            pred_index_set = set(self.ic_fix[pred])
            succ_index_set = set(self.ic_fix[succ])
            diff_index_set = succ_index_set.difference(pred_index_set)
            C = combinations_iterator( list(diff_index_set), weight_diff )
            for c in C:
                yield [self.ic_fix[pred] + c]
        
        elif i > 1:
            # get a fixed point associated to i-1
            FPI = self.fixed_points_iterator(i-1)
            for fp in FPI:
                fpp = self.ic_fix + fp  # tack on the initial data
                pred_index_set = set(fpp[pred])
                succ_index_set = set(fpp[succ])
                diff_index_set = succ_index_set.difference(pred_index_set)
                C = combinations_iterator( list(diff_index_set), weight_diff )
                for c in C:
                    yield fp+[fpp[pred]+c]
        else:
            yield None

    def compute_block_sizes(self):
        """Computes and stores the l[i], r[i] block sizes"""
        self.blocks_computed = True
        self.l = []
        self.r = []
        self.S = SFASchur(QQ)
        n = self.num_nodes
        for i in range(n):
            (pred, succ) = self.get_bounds(i)
            node_weight = self.weights[self.ic_size+i]
            pred_weight = self.weights[pred]
            succ_weight = self.weights[succ]
            self.l.append(node_weight - pred_weight)
            self.r.append(succ_weight - node_weight)

            self.schur_expand = dict()
            # expand polynomials for partitions in boxes which appear in
            # the tensor_term_iterator
            for P in PartitionsInBox(self.l[-1],self.r[-1]):
                f=self.S(P)
                g=self.S(conj(list(P)))
                self.schur_expand[(tuple(P),self.l[-1])] = f.expand(self.l[-1])
                self.schur_expand[(tuple(conj(list(P))),self.r[-1])] = g.expand(self.r[-1])

        # expand polynomials for the partitions in a box containing self.part
		# TODO: Doesn't seem to work with it's use in evaluate_schur
        for P in PartitionsInBox(len(self.part), self.part[0]):
            f=self.S(P)
            w=self.weights[self.ic_size-1]-self.weights[-1]
            self.schur_expand[(tuple(P),w)] = f.expand(w)

    def tensor_term_iterator(self):
        """Returns a coefficient, a list of S partitions, and a list of Q
        partitions which parametrize one term in the chern class of the
        tensor product of the bundles specified by the input data.
        INPUT: self
        OUTPUT: yields a ChernClassMonomial object
        """
        if self.blocks_computed == False:
            self.compute_block_sizes()
        FL_part = []
        n = self.num_nodes
        for i in range(n):
            FL_part.append(flagged_partition_iterator(self.l[i],self.r[i]))

        CPI = cartesian_product_iterator( [ list(a) for a in FL_part ] )
        for c in CPI:
            M = ChernClassMonomial()
            for i in range(n):
                (lam,mu) = c[i]
                M.append( ChernClass( dcoeff(lam,mu,self.l[i]), 's', i+1, mu ) )
                M.append( ChernClass( 1, 'q', i+1, conj( dual(lam,self.l[i],self.r[i]) )))
            yield M